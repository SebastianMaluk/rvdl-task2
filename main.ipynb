{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import configparser\n",
    "import numpy as np\n",
    "import models.siamese as siamese\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(\"config\", \"look.ini\"))\n",
    "config_model_dict = {k.upper(): v for k, v in config[\"SIAMESE\"].items()}\n",
    "config_data_dict = {k.upper(): v for k, v in config[\"DATA\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = int(config_data_dict[\"CROP_SIZE\"])\n",
    "def map_func(example_serialized):\n",
    "    print(example_serialized)  \n",
    "    image_anchor = example_serialized['anchor']\n",
    "    image_anchor = tf.image.resize_with_pad(image_anchor, 256, 256)\n",
    "    image_anchor = tf.image.random_crop(image_anchor, size = [crop_size, crop_size, 3])\n",
    "    image_anchor = tf.cast(image_anchor, tf.float32)\n",
    "\n",
    "    image_positive = example_serialized['positive']    \n",
    "    image_positive = tf.image.resize_with_pad(image_positive, 256, 256)\n",
    "    image_positive = tf.image.random_crop(image_positive, size = [crop_size, crop_size, 3])\n",
    "    image_positive = tf.cast(image_positive, tf.float32)\n",
    "\n",
    "    image_negative = example_serialized['negative']\n",
    "    image_negative = tf.image.resize_with_pad(image_negative, 256, 256)\n",
    "    image_negative = tf.image.random_crop(image_negative, size = [crop_size, crop_size, 3])\n",
    "    image_negative = tf.cast(image_negative, tf.float32)\n",
    "    return image_anchor, image_positive, image_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = int(config_data_dict[\"CROP_SIZE\"])\n",
    "def triplet_loss(alpha):\n",
    "    def loss(y_true, y_pred):\n",
    "        anchor, positive, negative = y_pred[:,:DIM], y_pred[:,DIM:2*DIM], y_pred[:,2*DIM:]\n",
    "        positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=1)\n",
    "        negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=1)\n",
    "        return tf.maximum(positive_dist - negative_dist + alpha, 0.)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 05:59:36.813406: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-09-30 05:59:36.813435: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2023-09-30 05:59:36.813444: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2023-09-30 05:59:36.813475: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-09-30 05:59:36.813492: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_known_ds, valid_unknown_ds = tfds.load(\n",
    "    \"sketchy\", split=[\"train\", \"validation_known\", \"validation_unknown\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anchor': <tf.Tensor 'args_0:0' shape=(256, 256, 3) dtype=uint8>, 'label': <tf.Tensor 'args_1:0' shape=() dtype=int64>, 'negative': <tf.Tensor 'args_2:0' shape=(256, 256, 3) dtype=uint8>, 'positive': <tf.Tensor 'args_3:0' shape=(256, 256, 3) dtype=uint8>}\n"
     ]
    }
   ],
   "source": [
    "AUTO = tf.data.AUTOTUNE\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .shuffle(1024)\n",
    "    .map(map_func, num_parallel_calls=AUTO)\n",
    "    .batch(int(config_model_dict[\"BATCH_SIZE\"]))\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 05:59:43.147731: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 74s 11s/step - loss: 1.0035 - dist_pos: 0.0533 - dist_neg: 0.0498\n",
      "Epoch 2/25\n",
      "4/4 [==============================] - 3s 740ms/step - loss: 1.0040 - dist_pos: 0.0555 - dist_neg: 0.0515\n",
      "Epoch 3/25\n",
      "4/4 [==============================] - 3s 723ms/step - loss: 1.0036 - dist_pos: 0.0523 - dist_neg: 0.0487\n",
      "Epoch 4/25\n",
      "4/4 [==============================] - 3s 738ms/step - loss: 1.0019 - dist_pos: 0.0487 - dist_neg: 0.0468\n",
      "Epoch 5/25\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 1.0002 - dist_pos: 0.0483 - dist_neg: 0.0481\n",
      "Epoch 6/25\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.9991 - dist_pos: 0.0504 - dist_neg: 0.0513\n",
      "Epoch 7/25\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.9986 - dist_pos: 0.0525 - dist_neg: 0.0539\n",
      "Epoch 8/25\n",
      "4/4 [==============================] - 3s 697ms/step - loss: 0.9977 - dist_pos: 0.0565 - dist_neg: 0.0588\n",
      "Epoch 9/25\n",
      "4/4 [==============================] - 3s 706ms/step - loss: 0.9975 - dist_pos: 0.0568 - dist_neg: 0.0593\n",
      "Epoch 10/25\n",
      "4/4 [==============================] - 3s 664ms/step - loss: 0.9984 - dist_pos: 0.0545 - dist_neg: 0.0561\n",
      "Epoch 11/25\n",
      "4/4 [==============================] - 3s 714ms/step - loss: 0.9990 - dist_pos: 0.0521 - dist_neg: 0.0531\n",
      "Epoch 12/25\n"
     ]
    }
   ],
   "source": [
    "model = siamese.Siamese(config_model_dict, config_data_dict)\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(momentum=0.9)\n",
    "model.compile(optimizer=optimizer, loss=triplet_loss(config_model_dict[\"ALPHA\"]))\n",
    "model.build()\n",
    "\n",
    "if (os.path.exists(\"model.weights.h5\")):\n",
    "  model.load_weights(\"model.weights.h5\")\n",
    "  history_dict = json.load(open(\"history.json\", \"r\"))\n",
    "else:\n",
    "  history = model.fit(train_ds, epochs=int(config_model_dict[\"EPOCHS\"]))\n",
    "  model.save_weights(\"model.weights.h5\")\n",
    "  history_dict = history.history\n",
    "  with open(\"history.json\", \"w\") as f:\n",
    "      json.dump(history_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.get_encoder()\n",
    "# Step 1: Generate Embeddings\n",
    "\n",
    "# Sketch embeddings and labels\n",
    "sketch_embeddings = []\n",
    "sketch_labels = []\n",
    "\n",
    "# Image embeddings and labels\n",
    "image_embeddings = []\n",
    "image_labels = []\n",
    "\n",
    "# Loop through the validation dataset and generate embeddings\n",
    "for example in tfds.as_numpy(valid_unknown_ds):  # Adjust based on actual use case\n",
    "    sketch = example['anchor']\n",
    "    image = example['positive']\n",
    "    label = example['label']\n",
    "    \n",
    "    sketch_embeddings.append(encoder.predict(np.expand_dims(sketch, axis=0))[0])\n",
    "    image_embeddings.append(encoder.predict(np.expand_dims(image, axis=0))[0])\n",
    "    \n",
    "    sketch_labels.append(label)\n",
    "    image_labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays for easier manipulation\n",
    "sketch_embeddings = np.array(sketch_embeddings)\n",
    "image_embeddings = np.array(image_embeddings)\n",
    "sketch_labels = np.array(sketch_labels)\n",
    "image_labels = np.array(image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "# Loop through all sketches and calculate predictions and ground truth\n",
    "for idx, query_embedding in enumerate(sketch_embeddings):\n",
    "    # Calculate distances to all image embeddings\n",
    "    distances = np.linalg.norm(image_embeddings - query_embedding, axis=1)\n",
    "    \n",
    "    # Since lower distance means higher similarity, use negative distance as prediction\n",
    "    predictions = -distances\n",
    "    \n",
    "    # Determine ground truth: 1 if image label matches sketch label, 0 otherwise\n",
    "    ground_truth = (image_labels == sketch_labels[idx]).astype(int)\n",
    "    \n",
    "    # Add to overall lists\n",
    "    all_predictions.append(predictions)\n",
    "    all_ground_truth.append(ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketchy_classes = json.load(open(os.path.join(\"tensorflow_datasets\", \"sketchy\", \"sketchy_classes.json\"), \"r\"))\n",
    "sketchy_classes = {int(v): k for k, v in sketchy_classes.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total mAP and mAP per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_precision(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate the Average Precision (AP) for a single class.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: List of predicted confidences\n",
    "    - ground_truth: List of ground truth labels\n",
    "    \n",
    "    Returns:\n",
    "    - average_precision: The average precision of the class\n",
    "    \"\"\"\n",
    "    # Sort predictions and ground_truth based on confidence scores\n",
    "    sorted_indices = np.argsort(predictions)[::-1]\n",
    "    sorted_ground_truth = ground_truth[sorted_indices]\n",
    "    \n",
    "    # Compute Precision-Recall curve\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    tp = 0  # true positive\n",
    "    fp = 0  # false positive\n",
    "    \n",
    "    for i, label in enumerate(sorted_ground_truth):\n",
    "        if label == 1:  # positive\n",
    "            tp += 1\n",
    "        else:  # negative\n",
    "            fp += 1\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / np.sum(ground_truth)  # assuming binary classification\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    # Compute Average Precision (AP)\n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    average_precision = np.sum(precisions * np.gradient(recalls))\n",
    "    \n",
    "    return average_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mAP across all queries\n",
    "all_average_precisions = [\n",
    "    calculate_average_precision(pred, gt)\n",
    "    for pred, gt in zip(all_predictions, all_ground_truth)\n",
    "]\n",
    "mean_average_precision = np.mean(all_average_precisions)\n",
    "print(f\"mAP: {mean_average_precision}\")\n",
    "\n",
    "# mAP for each class\n",
    "for i, ap in enumerate(all_average_precisions):\n",
    "    # get class name from label\n",
    "    class_name = sketchy_classes[sketch_labels[i]]\n",
    "    print(f\"AP for class {class_name}: {ap}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Recall@1 and Recall@1 per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_at_1(query_embedding, search_space_embeddings, query_label, search_space_labels):\n",
    "    \"\"\"\n",
    "    Calculate Recall@1.\n",
    "    \n",
    "    Parameters:\n",
    "    - query_embedding: The embedding of the query item (sketch)\n",
    "    - search_space_embeddings: The embeddings of items to search through (images)\n",
    "    - query_label: The class label of the query item\n",
    "    - search_space_labels: The class labels of the search items\n",
    "    \n",
    "    Returns:\n",
    "    - recall_at_1: Recall@1 score\n",
    "    \"\"\"\n",
    "    # Calculate distances between query and all items in search space\n",
    "    distances = np.linalg.norm(search_space_embeddings - query_embedding, axis=1)\n",
    "    \n",
    "    # Find the index of the nearest neighbor\n",
    "    nn_index = np.argmin(distances)\n",
    "    \n",
    "    # Check if the nearest neighbor has the same label as the query\n",
    "    return int(query_label == search_space_labels[nn_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recall_at_1 = []\n",
    "\n",
    "# Loop through all sketch embeddings and labels\n",
    "for query_embedding, query_label in zip(sketch_embeddings, sketch_labels):\n",
    "    \n",
    "    # Calculate Recall@1 for the current query\n",
    "    recall_at_1 = calculate_recall_at_1(query_embedding, image_embeddings, query_label, image_labels)\n",
    "    \n",
    "    # Append to the overall list\n",
    "    all_recall_at_1.append(recall_at_1)\n",
    "\n",
    "# Calculate the average Recall@1 across all queries\n",
    "average_recall_at_1 = np.mean(all_recall_at_1)\n",
    "\n",
    "print(f\"Average Recall@1: {average_recall_at_1:.2%}\")\n",
    "\n",
    "# Recall@1 for each class\n",
    "for i, recall_at_1 in enumerate(all_recall_at_1):\n",
    "    # get class name from label\n",
    "    class_name = sketchy_classes[sketch_labels[i]]\n",
    "    print(f\"Recall@1 for class {class_name}: {recall_at_1:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall-Precision Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Plot the Precision-Recall curve.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: Predicted confidence scores\n",
    "    - ground_truth: Ground truth labels\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(ground_truth, predictions)\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "# Loop through all sketches and calculate predictions and ground truth\n",
    "for idx, query_embedding in enumerate(sketch_embeddings):\n",
    "    # Calculate distances to all image embeddings\n",
    "    distances = np.linalg.norm(image_embeddings - query_embedding, axis=1)\n",
    "    \n",
    "    # Since lower distance means higher similarity, use negative distance as prediction\n",
    "    predictions = -distances\n",
    "    \n",
    "    # Determine ground truth: 1 if image label matches sketch label, 0 otherwise\n",
    "    ground_truth = (image_labels == sketch_labels[idx]).astype(int)\n",
    "    \n",
    "    # Add to overall lists\n",
    "    all_predictions.extend(predictions)\n",
    "    all_ground_truth.extend(ground_truth)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_ground_truth = np.array(all_ground_truth)\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plot_precision_recall_curve(all_predictions, all_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_examples(model, query_sketch, search_space_images, num_examples=10):\n",
    "    \"\"\"\n",
    "    Display example results.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained model to generate embeddings\n",
    "    - query_sketch: The sketch to use as a query (should be preprocessed in the same way as training data)\n",
    "    - search_space_images: The images to search through (should be a tf.data.Dataset object)\n",
    "    - num_examples: Number of examples to show\n",
    "    \n",
    "    Note: Adjust preprocessing and model prediction code as per your implementation.\n",
    "    \"\"\"\n",
    "    query_embedding = model.predict(np.expand_dims(query_sketch, axis=0), verbose=0)[0]\n",
    "\n",
    "    image_embeddings = []\n",
    "    image_labels = []\n",
    "    actual_images = []  # Store the actual images for later display\n",
    "\n",
    "    for example in tfds.as_numpy(search_space_images.take(1000)):\n",
    "        image = example['positive']\n",
    "        label = example['label']\n",
    "        image_embeddings.append(model.predict(np.expand_dims(image, axis=0), verbose=0)[0])\n",
    "        image_labels.append(label)\n",
    "        actual_images.append(image)  # Store the actual image\n",
    "\n",
    "    distances = np.linalg.norm(image_embeddings - query_embedding, axis=1)\n",
    "    closest_indices = np.argsort(distances)[:num_examples]\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.subplot(1, num_examples + 1, 1)\n",
    "    plt.imshow(query_sketch)\n",
    "    plt.title(\"Query\")\n",
    "\n",
    "    for i, idx in enumerate(closest_indices):\n",
    "        plt.subplot(1, num_examples + 1, i + 2)\n",
    "        plt.imshow(actual_images[idx])  # Use the stored actual images\n",
    "        plt.title(f\"Rank {i + 1}\\nLabel: {image_labels[idx]}\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sketches = 10\n",
    "\n",
    "for _ in range(num_sketches):\n",
    "    query_idx = np.random.randint(len(sketch_embeddings))\n",
    "    for i, example in enumerate(valid_unknown_ds):\n",
    "        if i == query_idx:\n",
    "            query_sketch = example['anchor']\n",
    "            break\n",
    "    display_examples(encoder, query_sketch, valid_unknown_ds, num_examples=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RVDL-i8K3yOnr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
